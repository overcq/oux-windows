//-*-C-*-
/*******************************************************************************
*   ___   publicplace
*  ¦OUX¦  C+
*  ¦/C+¦  component
*   ---   base
*         flow driver
* ©overcq                on "Gentoo Linux 13.0” “x86_64”             2015-1-26
*******************************************************************************/
// Programista "oux” rozumie nastêpuj¹ce zagadnienia podstawowe:
// ? prze³¹czanie ‹zadañ› wykonywane w pêtli dla kolejnych obudzonych jako ograniczon¹ liniowym czasem aktywacjê kolejnych warstw wykonawczej sieci informacyjnej (jak biologicznej sieci neuronowo-biochemicznej, inaczej elektryczno-molekularnej, inaczej synaptyczno-organicznej)
// ? informacyjno-wykonawcz¹ nienazywaj¹c¹ stwórczoœæ bytow¹ (molekularn¹) w postaci bezwarunkowej pêtli programowej odpowiednio u¿ytej
// ? pe³n¹ dynamicznoœæ systemu ‘uidów’ ‹raportów›/‹impulsatorów›, która jest odrêbna od u¿ywanego systemu zapewniania unikalnoœci ‘idów’ zarejestrowanych ‹raportów›/‹impulsatorów› u¿ywanych pomiêdzy ‹zadaniami›— jak rzeczywistoœæ wykonawcza nie jest zale¿na od nazw, ale zosta³a oparta na statycznych nazwach, bo tutaj program jest kompilowany ca³oœciowo, wiêc nie potrzeba uzgadniaæ ‘uidu’ ‹raportu›/‹impulsatora›
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Przeprojektowywane pod rzeczywiste potrzeby.
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// [ta linia– do aktualizacji.] Instrukcje mog¹ce prze³¹czyæ ‹zadanie›: “X_B”, [“X_E”], “Y_B”, “I_B”. “D_M” i “D_W” zawsze prze³¹czaj¹ jakby proceduralnie do ‹zadania› uruchamianego/zwalnianego, wykonuj¹ blok startowy/koñcowy i wracaj¹, wiêc nie zaliczaj¹ siê do tego zbioru.
// Bloki startowe i koñcowe ‹zadañ› s³u¿¹ obowi¹zkowo do rejestracji (np. ‹raportów›) i zwalniania zasobów ‘interfejsu’ bytu ‹zadania› ·bezwarunkowo· widzianych przez inne. W blokach tych nie wolno prze³¹czaæ (tzn. jawnie, nie przez normalne wykonywanie siê “D_W”) ani wykonywaæ celu ‹zadania›. (Jeœli w bloku startowym zostanie u¿yta jedna z instrukcji prze³¹czenia, to ‹zadanie› pozostanie zablokowane dla ‘schedulera’ w nie wznawialnym (nie obs³ugiwanym) stanie oczekiwania (“E_flow_Z_run_state_S_starting_by_task”). Jeœli zostanie u¿yta w bloku koñcowym, to ‹zadanie› zostanie zablokowane (“E_flow_Z_run_state_S_stopping_by_task”) i usuniête przez ‹zadanie› usuwaj¹ce, po prze³¹czeniu do niego przez ‘schedulera’— bez wykonania reszty programu ‹zadania› usuwanego.)
// Adresy danych dostêpnych przez identyfikatory (adresy ‹okien wgl¹du›) musz¹ byæ ponownie odczytane przed u¿yciem po mo¿liwym prze³¹czeniu ‹zadania›.
//------------------------------------------------------------------------------
// Zasad¹ jest, ¿e instrukcje oczekiwania na coœ przez ‹zadanie› odblokowuj¹ je dok³adnie po zaistnieniu tego czegoœ, bez zale¿noœci raportowania zaistnienia czegokolwiek, co ‹zadanie› ma zarejestrowane, ale czeka na to inn¹ instrukcj¹ prze³¹czaj¹c¹ ‹zadanie›. Wiêc oczekiwanie sekwencyjne na ró¿ne rzeczy przez ‹zadanie› jest sytuacj¹ obs³ugiwan¹ (tak jak powy¿ej napisane), aczkolwiek nie polecan¹.
// W³aœcicielem ‹raportu›/‹impulsatora› jest ‹zadanie›, które na niego czeka (rejestracja oczekiwania– “X_M”/“Yi_M”), i tylko to ‹zadanie› mo¿e czekaæ na ten ‹raport›/‹impulsator› (w dowolnej liczbie miejsc wewn¹trz w³asnej procedury); ale w ‘kompilacji’ ‹modu³ów› do “bibliotek” ‹zadanie› w ‹module› “main” —czyli w g³ównym pliku wykonywalnym programu— nie jest generatorem ‘uidu’ ‹raportu›/‹impulsatora›, który emituje któryœ ‹modu³› “biblioteczny”. W³aœcicielem ‚raportów znacznikowych’ —statycznych, nierejestrowanych, pochodz¹cych z “sygna³ów” ‘uniksowych’— jest ‹zadanie› “main” (a procedura), poniewa¿ tylko to ‹zadanie› mo¿e obs³ugiwaæ stany programu uruchomionego przez nie w sk³adnikach, a niektórych ‚raportów znacznikowych’– ‚scheduler’, poniewa¿ tylko on mo¿e zarz¹dzaæ systemowym przep³ywem wykonania i go prze³¹czaæ.
//------------------------------------------------------------------------------
// “process_call”– synchroniczne w przep³ywie pojedynczego ‹zadania›— funkcje ¿¹dane przy u¿yciu ‘uniksowego’ “sygna³u”:
// Tylko jedno ‹zadanie› w programie (‹sterownik› funkcji programu zewnêtrznego) mo¿e implementowaæ ¿¹dania funkcji konkretnego (jednego) programu "oux” uruchomionego w systemie operacyjnym. Obecnie w programie "bis” rozró¿nianie programów jest realizowane przez rozró¿nianie nazw procesów, tzn. nie jest uwzglêdniony przypadek, gdy program "oux” posiada procesy podrzêdne. [To zdanie sprawdziæ, bo coœ ju¿ by³o zrobione.] Ponadto nie zosta³o (dla optymalizacji) zaimplementowane rozpoznawanie programu typu "oux” (uzupe³niæ o autoryzacjê potwierdzan¹ chwilowo?) oraz w³asnego (wykluczaæ u Ÿród³a), a wykonanie ¿¹dania funkcji do takiego programu nie jest dozwolone.
//TODO Rozwi¹zaæ inaczej “subid” ‹zadañ› w w¹tku systemowym?
_internal
I _X_var( flow, call_req );
_internal
I _X_var( io, stream_write );
//==============================================================================
enum E_flow_Z_run_state
{ E_flow_Z_run_state_S_ready
, E_flow_Z_run_state_S_waiting_for_report
, E_flow_Z_run_state_S_waiting_for_timer
, E_flow_Z_run_state_S_waiting_for_call_reply
, E_flow_Z_run_state_S_starting_by_task
, E_flow_Z_run_state_S_stopping_by_task
};
struct E_flow_Q_task_Z
{ Pc exe_stack;
  N run_state_time;
  Pc stack;
  Pc touched_stack;
    #ifdef C_line_report
  Pc proc_name;
    #endif
  I run_state_object;
  N stack_size;
  HANDLE thread_flow_mutex;
  volatile B *thread_switch_back;
  P thread_proc_arg;
  void ( *thread_unblock_proc )(P);
  HANDLE thread;
  enum E_flow_Z_run_state run_state;
  unsigned U_R( type, system_unblock_report )   :1;
};
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct E_flow_Q_report_Z
{ N uid;
  N reported_count;
};
struct E_flow_Q_timer_Z
{ N left;
  N period;
  N lost_count;
  N uid;
  I task_to;
};
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
struct E_flow_Q_process_call_srv_Z
{ P shm;
  N process_id;
};
struct E_flow_Q_process_call_cli_Z
{ P shm;
  N process_id;
  int shm_id;
  I task_id;
  unsigned U_R( state, active )         :1;
  unsigned U_R( state, ping )           :1;
  unsigned U_R( state, reply )          :1;
};
//==============================================================================
N
E_flow_Q_task_I_granulation( void
){  if( !E_base_S->E_flow_Q_task_S )
        return 0;
    N granulation_u = E_simple_T_multiply_overflow( E_base_S->E_flow_Z_task_stacks_S_n_pages, E_base_S->E_mem_S_page_size )
    || E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size > E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1 //CONF
    ? E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1 //CONF
    : E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size;
    granulation_u /= E_mem_Q_tab_R_n( E_base_S->E_flow_Q_task_S );
    granulation_u = E_simple_Z_n_I_align_down_to_v2( granulation_u, E_base_S->E_mem_S_page_size );
    N ret = 0;
    for_each_out( 0, task_id_, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
    {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id_ );
        if( granulation_u < E_base_S->E_mem_S_page_size )
        {   ret--;
            continue;
        }
        if( task->stack_size > granulation_u )
            if( task->stack + task->stack_size - granulation_u <= task->touched_stack )
            {   N subtract = task->stack_size - granulation_u - E_base_S->E_mem_S_page_size;
                if(subtract)
                {   V0_( munmap( task->stack, subtract ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
            }else
            {   N subtract = task->touched_stack - E_base_S->E_mem_S_page_size - task->stack;
                if(subtract)
                {   V0_( munmap( task->stack, subtract ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
                ret--;
            }
    }
    return ret;
}
I
E_flow_Q_task_M( I *uid
, void (*task_proc)(P)
, P thread_proc_arg
, B task_in_thread_kind
    #ifdef C_line_report
, Pc task_proc_name
    #endif
){  N granulation_u = E_simple_T_multiply_overflow( E_base_S->E_flow_Z_task_stacks_S_n_pages, E_base_S->E_mem_S_page_size )
    || E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size > E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1
    ? E_simple_Z_n_I_mod_i2( ~0, sizeof(N) * 8 - 3 ) + 1 //CONF
    : E_base_S->E_flow_Z_task_stacks_S_n_pages * E_base_S->E_mem_S_page_size;
    granulation_u /= E_mem_Q_tab_R_n( E_base_S->E_flow_Q_task_S );
    granulation_u = E_simple_Z_n_I_align_down_to_v2( granulation_u, E_base_S->E_mem_S_page_size );
    if( granulation_u < E_base_S->E_mem_S_page_size )
    {   if( thread_proc_arg )
            W( thread_proc_arg );
        GV_(NA); // Late instrumentation error: granulation unit too small because too many tasks in available memory.
        return ~0;
    }
    for_each_out( 0, task_id_, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
    {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id_ );
        if( task->stack_size > granulation_u )
            if( task->stack + task->stack_size - granulation_u <= task->touched_stack )
            {   N subtract = task->stack_size - granulation_u - E_base_S->E_mem_S_page_size;
                if(subtract)
                {   V_( VirtualFree( task->stack, subtract, MEM_RELEASE ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
            }else
            {   GV_(NDFN); // Late instrumentation error: task stack larger than granulation unit.
                N subtract = task->touched_stack - E_base_S->E_mem_S_page_size - task->stack;
                if(subtract)
                {   V_( virtualFree( task->stack, subtract, MEM_RELEASE ));
                    task->stack += subtract;
                    task->stack_size -= subtract;
                }
            }
    }
    N stack_size = E_base_S->E_mem_S_page_size + granulation_u; // Jedna strona pamiêci “PROT_NONE” na pocz¹tku stosu.
    Pc stack;
    Pc exe_stack;
    N touched_size;
    Pc touched_stack;
    if( !task_in_thread_kind )
    {   V( stack = VirtualAlloc( 0
        , stack_size
        , MEM_RESERVE
        , PAGE_NOACCESS
        ))
        {   if( thread_proc_arg )
                W( thread_proc_arg );
            return ~0;
        }
        exe_stack = stack + stack_size;
        touched_size = E_base_S->E_mem_S_page_size;
        touched_stack = exe_stack - touched_size;
        V0( VirtualProtect( touched_stack
        , touched_size
        , PAGE_GUARD
        ))
        {   V_( VirtualFree( stack, 0, MEM_RELEASE ));
            if( thread_proc_arg )
                W( thread_proc_arg );
            return ~0;
        }
    }
    I task_id = E_mem_Q_tab_I_segv_add_begin( E_base_S->E_flow_Q_task_S );
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
    task->stack = stack;
    task->touched_stack = touched_stack;
        #ifdef C_line_report
    task->proc_name = task_proc_name;
        #endif
    task->run_state = E_flow_Z_run_state_S_starting_by_task;
    task->stack_size = stack_size;
    U_R( task->type, system_unblock_report ) = task_in_thread_kind;
    if( task_in_thread_kind )
    {   task->thread_proc_arg = thread_proc_arg;
        HANDLE thread_flow_mutex;
        V( thread_flow_mutex = CreateMutex( 0, FALSE, 0 ))
        {   _tasks_table_begin;
            E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _tasks_table_end;
            V_( VirtualFree( stack, 0, MEM_RELEASE ));
            if( thread_proc_arg )
                W( thread_proc_arg );
            return ~0;
        }
        B *M_( thread_switch_back );
        if( !thread_switch_back )
        {   V_( CloseHandle( thread_flow_mutex ));
            _tasks_table_begin;
            E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _tasks_table_end;
            V_( VirtualFree( stack, 0, MEM_RELEASE ));
            if( thread_proc_arg )
                W( thread_proc_arg );
            return ~0;
        }
        task->thread_flow_mutex = thread_flow_mutex;
        task->thread_switch_back = thread_switch_back;
        *thread_switch_back = no;
        I task_id_ = E_base_S->E_flow_Q_task_S_current;
        E_base_S->E_flow_Q_task_S_current = task_id;
        N thread_id;
        V( task->thread = CreateThread( 0, 0, E_flow_Q_task_in_thread_I_thread_proc, task_proc, 0, &thread_id ))
        {   E_base_S->E_flow_Q_task_S_current = task_id_;
            W( thread_switch_back );
            V_( CloseHandle( thread_flow_mutex ));
            _tasks_table_begin;
            E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _tasks_table_end;
            V_( VirtualFree( stack, 0, MEM_RELEASE ));
            if( thread_proc_arg )
                W( thread_proc_arg );
            return ~0;
        }
        O{  V_( SwitchToThread() );
            V_( WaitForSingleObject( thread_flow_mutex ));
            if( *thread_switch_back )
                break;
            Vr_( ReleaseMutex( thread_flow_mutex ));
        }
        task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
        if( task->run_state != E_flow_Z_run_state_S_ready )
        {   E_base_S->E_flow_Q_task_S_current = task_id_;
            Vr_( ReleaseMutex( thread_flow_mutex ));
            O{  V_( SwitchToThread() );
                N exit_code;
                V_( GetExitCodeThread( thread_flow_mutex, &exit_code ));
                if( exit_code != STILL_ACTIVE )
                    break;
            }
            V_( CloseHandle( thread_flow_mutex ));
            _tasks_table_begin;
            E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, task_id );
            _tasks_table_end;
            V_( VirtualFree( stack, 0, MEM_RELEASE ));
            if( thread_proc_arg )
                W( thread_proc_arg );
            return ~0;
        }
        *thread_switch_back = no;
        E_base_S->E_flow_Q_task_S_current = task_id_;
        return task_id;
    }
    task->exe_stack = 0;
    task->run_state_object = E_base_S->E_flow_Q_task_S_current;
    P *p = ( P * )exe_stack - 1;
    *p = (P)&E_flow_Q_task_I_stop;
    E_flow_Q_task_I_switch( task_id );
    if( !task->exe_stack ) // W bloku– nowe ‹zadanie›: nic nie zmieniaæ na stosie nale¿¹cym do prze³¹czanego.
    {   __asm__ volatile (
        #if defined( __i386__ ) || defined( __x86_64__ )
            #if defined( __i386__ )
        "\n" "mov   %0,%%esp"
            #else
        "\n" "mov   %0,%%rsp"
            #endif
        "\n" "jmp   *%1"
        //NKN Pozostaje zagwarantowanie, ¿e procedura ‹zadania› nie bêdzie nigdy oczekiwaæ (ze wzglêdu na ‘abi’) niczego tutaj nie zapewnianego przez ‘callera’ (m.in. rezerwacji obszaru na stosie, gdy argumentów i tak nie ma przekazywanych).
        :
        : "r" (p), "r" ( task_proc )
                #if defined( __i386__ )
        : "esp"
            #else
        : "rsp"
            #endif
        #else
#error not implemented
        #endif
        );
        _unreachable;
    }
    *uid = task_id;
    return task_id;
}
// Nie wolno tworzyæ “w¹tku”/‘instancji’ tego samego ‹zadania› w bloku startowym (innej ‘instancji’) tego samego ‹zadania›. Czyli nie wolno tworzyæ w ogóle w takim ‹zadaniu›, poniewa¿ ‹zadania› tworzy siê w bloku startowym.
I
E_flow_Q_task_M_thread( I *uid
, I subid
, void (*task_proc)(P)
, P thread_proc_arg
    #ifdef C_line_report
, Pc task_proc_name
    #endif
){  I uid_start = *uid;
    I id;
    if( !~( id = E_flow_Q_task_M( uid, task_proc, thread_proc_arg, yes
        #ifdef C_line_report
    , task_proc_name
        #endif
    )))
        return ~0;
    struct E_mem_Q_tab_Z **tab_subid;
    if( !~uid_start )
    {   *uid = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_task_S_uid_subid );
        tab_subid = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S_uid_subid, *uid );
        *tab_subid = E_mem_Q_tab_M( sizeof(I), subid + 1 );
        if( !*tab_subid )
        {   E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S_uid_subid, *uid );
            E_flow_Q_task_W( &id );
            return ~0;
        }
    }else
    {   tab_subid = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S_uid_subid, *uid );
        E_mem_Q_tab_I_add_i( E_base_S->E_flow_Q_task_S_uid_subid, subid );
    }
    *( I * )E_mem_Q_tab_R( *tab_subid, subid ) = id;
    return *uid;
}
// Procedura ‹zadañ› w w¹tku systemowym.
_internal
P
E_flow_Q_task_in_thread_I_thread_proc( P arg
){  Dh_();
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    WaitForSingleObject( task->thread_flow_mutex );
    void (*proc)(P);
    proc = arg;
    proc( task->thread_proc_arg );
    task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    ReleaseMutex( task->thread_flow_mutex );
    return 0;
}
// ‹Zadanie› wykonuje to przed rozpoczêciêm swojej pêtli; w instrukcji “I_D”.
B
E_flow_Q_task_I_begin( void
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->run_state = E_flow_Z_run_state_S_ready;
    if( U_R( task->type, system_unblock_report ))
    {   B *thread_switch_back = task->thread_switch_back;
        *thread_switch_back = yes;
        HANDLE thread_flow_mutex = task->thread_flow_mutex;
        V_( ReleaseMutex( thread_flow_mutex ));
        O{  V0_( SwitchToThread() );
            WaitForSingleObject( thread_flow_mutex );
            if( !*thread_switch_back )
                break;
            V_( ReleaseMutex( thread_flow_mutex ));
        }
    }else
        E_flow_Q_task_I_switch( task->run_state_object ); // Powrót do ‹zadania› uruchamiaj¹cego bie¿¹ce.
    task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
}
void
E_flow_Q_task_W( I *uid
){  I id = *uid;
    *uid = ~0;
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, id );
    task->run_state = E_flow_Z_run_state_S_stopping_by_task;
    task->run_state_object = E_base_S->E_flow_Q_task_S_current;
    if( U_R( task->type, system_unblock_report ))
    {   if( *task->thread_switch_back )
            task->thread_unblock_proc( task->thread_proc_arg );
        E_base_S->switch_back_try = no;
        HANDLE thread = task->thread;
        E_base_S->E_flow_Q_task_S_current = id;
        V_( ReleaseMutex( task->thread_flow_mutex ));
        O{  V_( SwitchToThread() );
            N exit_code;
            V_( GetExitCodeThread( thread_flow_mutex, &exit_code ));
            if( exit_code != STILL_ACTIVE )
                break;
        }
        task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, id );
        E_base_S->E_flow_Q_task_S_current = task->run_state_object;
        V_( CloseHandle( task->thread_flow_mutex ));
        W( task->thread_switch_back );
        if( task->thread_proc_arg )
            W( task->thread_proc_arg );
    }else
    {   E_flow_Q_task_I_switch(id); // Prze³¹cz tylko po to, by ‹zadanie› zwalniane zwolni³o zasoby; równie¿ stosowo, hierarchicznie z powrotem prze³¹czaj¹c przy zwalnianiu ‹zadañ› przez siebie uruchomionych.
        task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, id );
    }
    E_flow_Q_task_I_touch_stack();
    V_( VirtualFree( task->stack, 0, MEM_RELEASE ));
    E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_task_S, id );
}
void
E_flow_Q_task_W_thread( I *uid
, I subid
){  struct E_mem_Q_tab_Z **tab_subid = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S_uid_subid, *uid );
    I id = *( I * )E_mem_Q_tab_R( *tab_subid, subid );
    if( E_mem_Q_tab_R_n( *tab_subid ) != 1 )
        E_mem_Q_tab_I_remove( *tab_subid, subid );
    else
    {   E_mem_Q_tab_W( *tab_subid );
        *uid = ~0;
    }
    E_flow_Q_task_W(&id);
}
// ‹Zadanie› wykonuje to po wyjœciu z procedury; ma adres powrotu na stosie.
_internal
void
E_flow_Q_task_I_stop( void
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    E_flow_Q_task_I_switch( task->run_state_object ); // Powrót do ‹zadania› zwalniaj¹cego bie¿¹ce.
    _unreachable;
}
//------------------------------------------------------------------------------
// Wymuszenie ‘prealokacji’ “stosu” ‹zadania›, by nie zosta³a wywo³ana procedura obs³ugi “sygna³u” ‘uniksowego’ “SEGV” w trakcie zmieniania pamiêci ‘alokowanej’ dynamicznie, z której korzysta.
_internal
void
E_flow_Q_task_I_touch_stack( void
){  if( !E_base_S->E_flow_Q_task_S_current ) // ‹Zadanie› inne ni¿ “main”, poniewa¿ to ma stos zarz¹dzany przez system operacyjny.
        return;
    N pages = 4; //CONF
    if( !pages )
    {   GV_(NDFN); V();
    }
    volatile Pc sp;
    __asm__ volatile (
    #if defined( __i386__ ) || defined( __x86_64__ )
        #if defined( __i386__ )
    "\n" "mov   %%esp,%0"
        #else
    "\n" "mov   %%rsp,%0"
        #endif
    #else
#error not implemented
    #endif
    : "=r" (sp)
    );
    // Dlatego kolejno “strony” pamiêci, poniewa¿ jest tylko jedna zabezpieczaj¹ca na dole “stosu” ‹zadania›.
    while( pages-- )
    {   sp -= E_base_S->E_mem_S_page_size;
        *sp = 0;
    }
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
I
E_flow_Q_report_M( N uid
){  struct E_flow_Q_report_Z *report;
    for_each( report_id, E_base_S->E_flow_Q_report_S, E_mem_Q_tab )
    {   report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, report_id );
        if( report->uid == uid )
            break;
    }
    if( !~report_id )
    {   report_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_report_S );
        report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, report_id );
        report->uid = uid;
        report->reported_count = 0;
    }
    return report_id;
}
void
E_flow_Q_report_W( I id
){  E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_report_S, id );
}
//------------------------------------------------------------------------------
void
E_flow_Q_report_I_signal( I id
){  struct E_flow_Q_report_Z *report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, id );
    if( ~report->reported_count )
        report->reported_count++;
    for_each( task_id, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
    {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
        if( task->run_state == E_flow_Z_run_state_S_waiting_for_report
        && task->run_state_object == id
        )
        {   task->run_state = E_flow_Z_run_state_S_ready;
            break;
        }
    }
}
B
E_flow_Q_report_I_wait( I id
, N *lost_count
){  B ret;
    struct E_flow_Q_report_Z *report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, id );
    if( report->reported_count )
    {   //for_each( task_id, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
        //{   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
            //if( task->run_state == E_flow_Z_run_state_S_emiting_report
            //&& task->run_state_object == id
            //)
                //task->run_state = E_flow_Z_run_state_S_ready;
        //}
        ret = no; // Nie wywo³uje “schedule”, poniewa¿ w prze³¹czanym tylko w oznaczonych punktach przep³ywie wykonania— bie¿¹ce ‹zadanie› mog³o umo¿liwiæ zaistnienie ‹raportu›, na który czeka, tylko wtedy, jeœli prze³¹cza do innych ‹zadañ› ·w innych punktach ni¿ to oczekiwanie na ‹raport›·, wiêc po co czekaæ, skoro nie zaburza cyklu prze³¹czania ‹zadañ›, a tylko w implementacji w³asnego ‹zadania› zmienia na z³o¿on¹ (przesuniêt¹) sekwencjê prze³¹czania.
    }else
    {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
        task->run_state = E_flow_Z_run_state_S_waiting_for_report;
        task->run_state_object = id;
        ret = E_flow_Q_task_I_schedule();
        report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, id );
    }
    if( lost_count )
        *lost_count = report->reported_count - 1;
    report->reported_count = 0;
    return ret;
}
void
E_flow_Q_report_I_clear( I id
){  struct E_flow_Q_report_Z *report = E_mem_Q_tab_R( E_base_S->E_flow_Q_report_S, id );
    report->reported_count = 0;
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
I
E_flow_Q_timer_M( N period
){  Z_clock_time tv;
    _gettime( &tv );
    I timer_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_timer_S );
    struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
    timer->period.tv_sec = period / 1000000;
    timer->period.Z_clock_time_minor_field = period % 1000000;
        #ifdef E_flow_drv_C_clock_monotonic
    timer->period.Z_clock_time_minor_field *= 1000;
        #endif
    timer->lost_count = 0;
    timer->uid = ~0;
    timer->task_to = E_base_S->E_flow_Q_task_S_current;
    if( E_mem_Q_tab_R_n( E_base_S->E_flow_Q_timer_S ) != 1 )
    {   for_each_out( timer_id, timer_id_, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
        {   struct E_flow_Q_timer_Z *timer_ = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id_ );
            if( _timerisset( &timer_->period ) // Jest co najmniej jeden ‹cykler›.
            || _timerisset( &timer_->left ) // Jest co najmniej jeden wzbudzony ‹impulsator›.
            )
            {   _timeradd( &tv, &timer->period, &tv );
                if( _timercmp( &tv, <, &E_base_S->E_flow_Q_timer_S_next_real_time ))
                    E_base_S->E_flow_Q_timer_S_next_real_time = tv;
                _timersub( &tv, &E_base_S->E_flow_Q_timer_S_last_real_time, &timer->left );
                return timer_id;
            }
        }
    }
    E_base_S->E_flow_Q_timer_S_last_real_time = tv;
    _timeradd( &tv, &timer->period, &E_base_S->E_flow_Q_timer_S_next_real_time );
    timer->left = timer->period;
    return timer_id;
}
void
E_flow_Q_timer_W( I id
){  E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_timer_S, id );
    for_each( timer_id, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
    {   struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        if( _timerisset( &timer->period )
        || _timerisset( &timer->left )
        )
            return;
    }
    _timerover( &E_base_S->E_flow_Q_timer_S_next_real_time );
}
//------------------------------------------------------------------------------
B
E_flow_Q_timer_I_wait( I id
, N *lost_count
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->run_state = E_flow_Z_run_state_S_waiting_for_timer;
    task->run_state_object = id;
    B ret = E_flow_Q_task_I_schedule();
    struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, id );
    if( lost_count )
        *lost_count = timer->lost_count;
    timer->lost_count = 0;
    return ret;
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
I
E_flow_Q_impulser_M( N uid
){  struct E_flow_Q_timer_Z *timer;
    for_each( timer_id, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
    {   timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        if( timer->uid == uid )
            break;
    }
    if( !~timer_id )
    {   timer_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_timer_S );
        timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        _timerclear( &timer->left );
        _timerclear( &timer->period );
        timer->uid = uid;
    }
    timer->task_to = E_base_S->E_flow_Q_task_S_current;
    return timer_id;
}
I
E_flow_Q_impulser_M_srv( N uid
){  struct E_flow_Q_timer_Z *timer;
    for_each( timer_id, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
    {   timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        if( timer->uid == uid )
            break;
    }
    if( !~timer_id )
    {   timer_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_timer_S );
        timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
        _timerclear( &timer->left );
        _timerclear( &timer->period );
        timer->uid = uid;
    }
    return timer_id;
}
//------------------------------------------------------------------------------
void
E_flow_Q_impulser_I_activate( I id
, N time
){  Z_clock_time tv;
    _gettime( &tv );
    struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, id );
    timer->left.tv_sec = time / 1000000;
    timer->left.Z_clock_time_minor_field = time % 1000000;
        #ifdef E_flow_drv_C_clock_monotonic
    timer->left.Z_clock_time_minor_field *= 1000;
        #endif
    if( E_mem_Q_tab_R_n( E_base_S->E_flow_Q_timer_S ) != 1 )
    {   for_each_out( id, timer_id_, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
        {   struct E_flow_Q_timer_Z *timer_ = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id_ );
            if( _timerisset( &timer_->period )
            || _timerisset( &timer_->left )
            )
            {   _timeradd( &tv, &timer->left, &tv );
                if( _timercmp( &tv, <, &E_base_S->E_flow_Q_timer_S_next_real_time ))
                    E_base_S->E_flow_Q_timer_S_next_real_time = tv;
                _timersub( &tv, &E_base_S->E_flow_Q_timer_S_last_real_time, &timer->left );
                return;
            }
        }
    }
    E_base_S->E_flow_Q_timer_S_last_real_time = tv;
    _timeradd( &tv, &timer->left, &E_base_S->E_flow_Q_timer_S_next_real_time );
}
void
E_flow_Q_impulser_I_deactivate( I id
){  struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, id );
    if( !_timerisset( &timer->left ))
        return;
    _timerclear( &timer->left );
    for_each_out( id, timer_id_, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
    {   struct E_flow_Q_timer_Z *timer_ = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id_ );
        if( _timerisset( &timer_->period )
        || _timerisset( &timer_->left )
        )
            return;
    }
    _timerover( &E_base_S->E_flow_Q_timer_S_next_real_time );
}
//------------------------------------------------------------------------------
//NDFN Dodaæ “lost_count”?
B
E_flow_Q_impulser_I_wait( I id
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->run_state = E_flow_Z_run_state_S_waiting_for_timer;
    task->run_state_object = id;
    return E_flow_Q_task_I_schedule();
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
void
E_flow_Q_thread_system_unblock_report_M( void ( *thread_unblock_proc )(P)
, pthread_mutex_t **thread_flow_mutex
, B **thread_switch_back
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->thread_unblock_proc = thread_unblock_proc;
    *thread_flow_mutex = task->thread_flow_mutex;
    *thread_switch_back = task->thread_switch_back;
}
void
E_flow_Q_thread_system_unblock_report_I_before_block(
  pthread_mutex_t *thread_flow_mutex
){  E_flow_Q_task_I_touch_stack();
    Vr_( pthread_sigmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply, 0 ));
    Vr_( pthread_mutex_unlock( thread_flow_mutex ));
}
B
E_flow_Q_thread_system_unblock_report_I_after_block(
  pthread_mutex_t *thread_flow_mutex
){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, 0 );
    while( E_base_S->switch_back_try )
    {   V0_( sched_yield() );
    }
    while( E_base_S->switch_back_suspend )
    {   Vr_( pthread_kill( task->thread, SIGALRM ));
        V0_( sched_yield() );
    }
    Vr_( pthread_mutex_lock( thread_flow_mutex ));
    Vr_( pthread_sigmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply, 0 ));
    task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
}
void
E_flow_Q_thread_system_unblock_report_I_unblock( I task_uid
, I task_subid
){  struct E_mem_Q_tab_Z **tab_subid = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S_uid_subid, task_uid );
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, *( I * )E_mem_Q_tab_R( *tab_subid, task_subid ));
    task->thread_unblock_proc( task->thread_proc_arg );
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// Ka¿de ‹zadanie› po wywo³aniu “E_flow_Q_task_I_schedule” i po prze³¹czeniu w tej procedurze do innego ‹zadania› (“E_flow_Q_task_I_switch”) czeka przed instrukcj¹ powrotu z tej procedury, by kontynuowaæ w miejscu wywo³ania i ewentualnie zakoñczyæ w³asne ‹zadanie› po powrocie.
__attribute__ ((__noinline__,__returns_twice__,__hot__))
B
E_flow_Q_task_I_schedule( void
){  _forced_statement;
    I schedule_task_id = E_base_S->E_flow_Q_task_S_current;
Loop:
    O{  if( U_E( E_base_S->E_flow_S_signal, exit ))
        {
                #ifdef C_middle_code
            U_F( E_base_S->E_flow_S_signal, exit_all );
                #endif
            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, 0 );
            task->run_state = E_flow_Z_run_state_S_stopping_by_task;
            if( E_base_S->E_flow_Q_task_S_current != schedule_task_id )
                E_base_S->E_flow_Q_task_S_current = schedule_task_id;
            if( E_base_S->E_flow_Q_task_S_current )
            {   E_flow_Q_task_I_switch(0);
                task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
                return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
            }
            return yes;
        }
        if( U_E( E_base_S->E_flow_S_signal, call_req ))
            X_F( flow, call_req );
        if( U_E( E_base_S->E_flow_S_signal, io_ready ))
            X_F( io, stream_write );
        Z_clock_time next_real_time;
        _timerover( &next_real_time );
        Z_clock_time tv;
        _gettime( &tv );
        for_each( call_id, E_base_S->E_flow_Q_process_call_cli_S, E_mem_Q_tab )
        {   struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, call_id );
            if( !U_R( call->state, active ))
                continue;
            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, call->task_id );
            if( _timercmp( &tv, <, &task->run_state_time ))
            {   if( _timercmp( &task->run_state_time, <, &next_real_time ))
                    next_real_time = task->run_state_time;
            }else if( U_R( call->state, ping ))
            {   task->run_state = E_flow_Z_run_state_S_ready;
                U_L( call->state, active );
            }else
            {   U_F( call->state, ping );
                if( kill( call->process_id, SIGVTALRM ))
                {   task->run_state = E_flow_Z_run_state_S_ready;
                    U_L( call->state, active );
                }else
                {   _timeradd( &tv, &E_base_S->E_flow_Q_process_call_S_ping_period, &task->run_state_time );
                    if( _timercmp( &task->run_state_time, <, &next_real_time ))
                        next_real_time = task->run_state_time;
                }
            }
        }
        if( U_E( E_base_S->E_flow_S_signal, time ) // Na pewno poni¿sza linia, bo przychodzi z obudzenia po up³ywie tego czasu.
        || !_timercmp( &tv, <, &E_base_S->E_flow_Q_timer_S_next_real_time ) // Jeœli nie przychodzi z obudzenia, a z wywo³ania w którymœ ‹zadaniu›, to trzeba sprawdziæ najbli¿szy czas ‹cyklerów>.
        ) // Czy trzeba uaktualniæ kolejne czasy ‹cyklerów›.
        {   Z_clock_time elapsed_time;
            _timersub( &tv, &E_base_S->E_flow_Q_timer_S_last_real_time, &elapsed_time );
            //NDFN Uzupe³niæ o jakieœ przewidywanie ‘overhead’ na podstawie poprzedniego, by wyeliminowaæ mo¿liwoœæ powtarzania pêtli w pesymistycznym przypadku dla ka¿dego ‹cyklera›? Ale obliczaæ ten czas tylko wtedy, je¿eli ten fragment nie bêdzie móg³ byæ wyw³aszczony z wykonywania w czasie rzeczywistym (wszystkie przerwania wy³¹czone).
            O{  B some_timer_is_active = no, some_timer_has_deactivated = no;
                B some_task_got_ready = no;
                Z_clock_time suspend_time;
                _timerover( &suspend_time );
                for_each( timer_id, E_base_S->E_flow_Q_timer_S, E_mem_Q_tab )
                {   struct E_flow_Q_timer_Z *timer = E_mem_Q_tab_R( E_base_S->E_flow_Q_timer_S, timer_id );
                    if( _timerisset( &timer->period )) // ‹cykler›.
                    {   if( !_timercmp( &elapsed_time, <, &timer->left )) // ‹cykler› wykona³ obieg— ‹zadanie› do wznowienia.
                        {   Z_clock_time overlate_time;
                            _timersub( &elapsed_time, &timer->left, &overlate_time );
                            if( !_timercmp( &overlate_time, <, &timer->period )) // ‹cykler› wykona³ wiêcej ni¿ jeden obieg.
                            {   do
                                {   timer->lost_count++;
                                    _timersub( &overlate_time, &timer->period, &overlate_time );
                                }while( !_timercmp( &overlate_time, <, &timer->period ));
                                GV_(NA); Gd( timer->lost_count ); Gd( timer->period.tv_sec ); Gd( timer->period.Z_clock_time_minor_field ); Gd( overlate_time.tv_sec ); Gd( overlate_time.Z_clock_time_minor_field ); // lost time.
                            }
                            _timersub( &timer->period, &overlate_time, &timer->left );
                            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, timer->task_to );
                            if( task->run_state == E_flow_Z_run_state_S_waiting_for_timer
                            && task->run_state_object == timer_id
                            )
                            {   task->run_state = E_flow_Z_run_state_S_ready;
                                some_task_got_ready = yes;
                            }else
                                timer->lost_count++;
                        }else
                            _timersub( &timer->left, &elapsed_time, &timer->left );
                        if( _timercmp( &timer->left, <, &suspend_time ))
                        {   suspend_time = timer->left;
                            some_timer_is_active = yes;
                        }
                    }else if( _timerisset( &timer->left )) // Aktywowany ‹impulsator›.
                    {   if( !_timercmp( &elapsed_time, <, &timer->left ))
                        {   _timerclear( &timer->left );
                            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, timer->task_to );
                            if( task->run_state == E_flow_Z_run_state_S_waiting_for_timer
                            && task->run_state_object == timer_id
                            )
                            {   task->run_state = E_flow_Z_run_state_S_ready;
                                some_task_got_ready = yes;
                            } // Narazie– jeœli nie mo¿e wznowiæ ‹zadania›, to gubi impuls.
                            some_timer_has_deactivated = yes;
                        }else
                        {   _timersub( &timer->left, &elapsed_time, &timer->left );
                            if( _timercmp( &timer->left, <, &suspend_time ))
                            {   suspend_time = timer->left;
                                some_timer_is_active = yes;
                            }
                        }
                    }
                }
                if( some_timer_has_deactivated
                && !some_timer_is_active
                ){  _timerover( &E_base_S->E_flow_Q_timer_S_next_real_time );
                    break;
                }
                Z_clock_time tv_2;
                _gettime( &tv_2 );
                _timersub( &tv_2, &tv, &elapsed_time );
                if( _timercmp( &elapsed_time, <, &suspend_time ) // Czy przeliczanie czasów ‹cyklerów› trwa³o krócej ni¿ obliczony czas oczekiwania do pierwszego budz¹cego ‹zadanie›?
                || !some_task_got_ready
                ){  E_base_S->E_flow_Q_timer_S_last_real_time = tv;
                    _timeradd( &tv, &suspend_time, &E_base_S->E_flow_Q_timer_S_next_real_time ); //NDFN Rozwa¿yæ przepe³nienie licznika czasu rzeczywistego.
                    break;
                }
                tv = tv_2;
                GV_(NA); Gd( suspend_time.tv_sec ); Gd( suspend_time.Z_clock_time_minor_field ); Gd( elapsed_time.tv_sec ); Gd( elapsed_time.Z_clock_time_minor_field ); // Timer schedule loop overhead.
            }
        }
        for_each_out( E_base_S->E_flow_Q_task_S_current, task_id, E_base_S->E_flow_Q_task_S, E_mem_Q_tab )
        {   struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_id );
            if( U_R( task->type, system_unblock_report ))
            {   B *thread_switch_back = task->thread_switch_back;
                if( !*thread_switch_back )
                {   E_base_S->E_flow_Q_task_S_current = task_id;
                    pthread_mutex_t *thread_flow_mutex = task->thread_flow_mutex;
                    Vr_( pthread_sigmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply, 0 ));
                    Vr_( pthread_mutex_unlock( thread_flow_mutex ));
                    O{  V0_( sched_yield() );
                        Vr_( pthread_mutex_lock( thread_flow_mutex ));
                        E_base_S->switch_back_try = yes;
                        if( *thread_switch_back )
                            break;
                        E_base_S->switch_back_try = no;
                        Vr_( pthread_mutex_unlock( thread_flow_mutex ));
                    }
                    Vr_( pthread_sigmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply, 0 ));
                    V0_( sched_yield() ); // ¯eby daæ szansê na wykonanie siê “w¹tku” ‹zadania›, z którego prze³¹czono spowrotem— od “Xh_B_” do rzeczywistej procedury systemowej, na której siê blokuje.
                    U_L( E_base_S->E_flow_S_signal, time ); // “kill” z “Xh_B” jest niepotrzebne, wiêc ‘resetuje’ stan, by nie potrzeba by³o póŸniej wykonaæ ponownej pêtli “schedule” ze wzglêdu na stan “wake”.
                    goto Loop; // Wywo³uje “E_flow_Q_task_I_schedule” w sposób wewnêtrzny, gdy nie mo¿na u¿yæ funkcjonalnoœci “E_flow_Q_task_I_switch”, ale przy pierwszej koniecznoœci prze³¹czenia standardowego bêdzie prze³¹cza³ w trybie ·pominiêcia nie ob³ugiwanych systemowych danych zatrzymywania przep³ywu wykonania z tablicy ‹zadañ›· (pominiêcia ‹zadañ›) przekazanych kolejnych typu “w¹tkowanych” ‹systemowych raportów odblokowuj¹cych› z “cyklicznej kolejki”— na rzecz zwyk³ego ‹zadania› (“schedule_task_id”), które wywo³a³o “schedule”.
                }
            }else
            {   if( task->run_state == E_flow_Z_run_state_S_ready )
                {
                        #ifdef E_flow_C_thread_system_unblock_reports
                    if( E_base_S->E_flow_Q_task_S_current != schedule_task_id )
                        E_base_S->E_flow_Q_task_S_current = schedule_task_id;
                    if( task_id != E_base_S->E_flow_Q_task_S_current )
                        #endif
                    {   E_flow_Q_task_I_switch( task_id );
                        task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
                    }
                    return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
                }
            }
        }
        struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
        if( U_R( task->type, system_unblock_report ))
        {   B *thread_switch_back = task->thread_switch_back;
            if( !*thread_switch_back )
            {   pthread_mutex_t *thread_flow_mutex = task->thread_flow_mutex;
                Vr_( pthread_sigmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply, 0 ));
                Vr_( pthread_mutex_unlock( thread_flow_mutex ));
                O{  V0_( sched_yield() );
                    Vr_( pthread_mutex_lock( thread_flow_mutex ));
                    E_base_S->switch_back_try = yes;
                    if( *thread_switch_back )
                        break;
                    E_base_S->switch_back_try = no;
                    Vr_( pthread_mutex_unlock( thread_flow_mutex ));
                }
                Vr_( pthread_sigmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply, 0 ));
                V0_( sched_yield() );
                U_L( E_base_S->E_flow_S_signal, time );
                continue;
            }
        }else
            if( task->run_state == E_flow_Z_run_state_S_ready )
            {   if( E_base_S->E_flow_Q_task_S_current != schedule_task_id )
                    E_base_S->E_flow_Q_task_S_current = schedule_task_id;
                return task->run_state == E_flow_Z_run_state_S_stopping_by_task;
            }
        if( U_R( E_base_S->E_flow_S_signal, exit )
        || U_R( E_base_S->E_flow_S_signal, time )
        || U_R( E_base_S->E_flow_S_signal, call_req )
        || U_R( E_base_S->E_flow_S_signal, io_ready )
        )
            continue;
        if( _timercmp( &E_base_S->E_flow_Q_timer_S_next_real_time, <, &next_real_time ))
            next_real_time = E_base_S->E_flow_Q_timer_S_next_real_time;
        B U_has_suspend_time = !_timerisover( &next_real_time );
        if( U_has_suspend_time )
            _gettime( &tv );
        if( !U_has_suspend_time
        || _timercmp( &tv, <, &next_real_time )
        ){  if( U_has_suspend_time )
            {   _timersub( &next_real_time, &tv, &tv );
                _setttimer( &tv );
            }
            if( !U_R( E_base_S->E_flow_S_signal, exit )
            && !U_R( E_base_S->E_flow_S_signal, time )
            && !U_R( E_base_S->E_flow_S_signal, call_req )
            && !U_R( E_base_S->E_flow_S_signal, io_ready )
            )
                U_E( E_base_S->E_flow_S_signal, wake );
                while( !U_R( E_base_S->E_flow_S_signal, wake ))
                {
                        #ifdef E_flow_C_thread_system_unblock_reports
                    E_base_S->switch_back_suspend = yes;
                    E_base_S->switch_back_try = no;
                        #endif
                    Vp_(( sigsuspend( &E_base_S->E_flow_Z_sigset_S_empty ), !_errno || _errno == EINTR ));
                        #ifdef E_flow_C_thread_system_unblock_reports
                    E_base_S->switch_back_suspend = no;
                        #endif
                }
            if( U_has_suspend_time
            && !U_R( E_base_S->E_flow_S_signal, time )
            ){  Z_clock_time tv_;
                _timerclear( &tv_ );
                _setttimer( &tv_ );
            }
        }
    }
}
// Prze³¹czenie do ‹zadania› przez prze³¹czenie wskaŸnika “stosu” wykonania.
__attribute__ ((__noinline__,__returns_twice__,__hot__))
_internal
void
E_flow_Q_task_I_switch( I task_to_id
){
//CONF Jest ‘fpu’?
        #if 1
    feclearexcept( FE_ALL_EXCEPT );
        #endif
    struct E_flow_Q_task_Z *task_from = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    struct E_flow_Q_task_Z *task_to = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, task_to_id );
    E_base_S->E_flow_Q_task_S_current = task_to_id;
    __asm__ volatile (
    #if defined( __i386__ ) || defined( __x86_64__ )
        #if defined( __i386__ )
    "\n" "mov       %%esp,%0"
    "\n" "test      %1,%1"
//CONF jest ‘cmov’?
            #if 1
    "\n" "cmovne    %1,%%esp"
            #else
    "\n" "jz        0f"
    "\n" "mov       %1,%%esp"
    "\n" "0:"
            #endif
        #else
    "\n" "mov       %%rsp,%0"
    "\n" "cmp       $0,%1"
    "\n" "cmovne    %1,%%rsp"
        #endif
    : "=m" ( task_from->exe_stack )
    : "r" ( task_to->exe_stack )
    : "cc", "memory"
//CONF Jest ‘fpu’?
        #if 1
    , "st", "st(1)", "st(2)", "st(3)", "st(4)", "st(5)", "st(6)", "st(7)"
            #ifdef __MMX__
    , "mm0", "mm1", "mm2", "mm3", "mm4", "mm5", "mm6", "mm7"
                #ifdef __SSE__
    , "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5", "xmm6", "xmm7"
                #endif
            #endif
        #endif
        #if defined( __i386__ )
    , "ebx", "ecx", "ebp", "esi", "edi"
        #else
    , "rbx", "rcx", "rbp", "rsi", "rdi"
    , "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15"
            #ifdef __SSE3__
    , "xmm8", "xmm9", "xmm10", "xmm11", "xmm12", "xmm13", "xmm14", "xmm15"
                #ifdef __AVX__
    , "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm5", "ymm6", "ymm7", "ymm8", "ymm9", "ymm10", "ymm11", "ymm12", "ymm13", "ymm14", "ymm15"
                #endif
            #endif
        #endif
    #else
#error not implemented
    #endif
    );
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_internal
void
E_flow_Z_signal_V_segv( int uid
, siginfo_t *siginfo
, P context
){  Dh_();
    int errno_ = _errno;
    if( E_base_S->E_flow_Q_task_S_current
        #ifdef C_pthreads
    && U_R( E_base_S->E_flow_S_mode, Z_task_table_S_can_read )
        #endif
    ){  struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
        if( (Pc)siginfo->si_addr < task->touched_stack )
            if( (Pc)siginfo->si_addr >= task->stack + E_base_S->E_mem_S_page_size )
            {   Pc new_base = E_simple_Z_p_I_align_down_to_v2( siginfo->si_addr, E_base_S->E_mem_S_page_size );
                V0_( mprotect( new_base
                , task->touched_stack - new_base
                , PROT_READ | PROT_WRITE
                ));
                task->touched_stack = new_base;
                _errno = errno_;
                return;
            }else if( (Pc)siginfo->si_addr >= task->stack )
            {   GV_(NA); // Late instrumentation error: task stack too small.
            }
    }
    GV_(NDFN); Gd( siginfo->si_code ); Gh( siginfo->si_addr ); // Other fault.
    struct sigaction sa =
    { .sa_handler = SIG_DFL
    , .sa_flags = 0
    };
    V0_( sigaction( SIGSEGV, &sa, 0 ));
    _errno = errno_;
}
//------------------------------------------------------------------------------
_internal
void
E_flow_Z_signal_V_process_call_ping( int uid
, siginfo_t *siginfo
, P data
){  Dh_();
    if( siginfo->si_code != SI_USER )
        return;
    int errno_ = _errno;
    V0( kill( siginfo->si_pid, SIGPROF )){}
    _errno = errno_;
}
_internal
void
E_flow_Z_signal_V_process_call_pong( int uid
, siginfo_t *siginfo
, P data
){  Dh_();
    if( siginfo->si_code != SI_USER )
        return;
    int errno_ = _errno;
    for_each( call_id, E_base_S->E_flow_Q_process_call_cli_S, E_mem_Q_tab )
    {   struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, call_id );
        if( call->process_id == siginfo->si_pid )
        {   U_L( call->state, ping );
            U_F( E_base_S->E_flow_S_signal, wake );
            break;
        }
    }
    _errno = errno_;
}
_internal
void
E_flow_Z_signal_V_process_call_req( int uid
, siginfo_t *siginfo
, P data
){  Dh_();
        #ifndef E_flow_Q_process_call_C_alt
    if( siginfo->si_code != SI_QUEUE )
        #else
    if( siginfo->si_code != SI_USER )
        #endif
        return;
    int errno_ = _errno;
    for_each( call_id, E_base_S->E_flow_Q_process_call_srv_S, E_mem_Q_tab )
    {   struct E_flow_Q_process_call_srv_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_srv_S, call_id );
        if( call->process_id == siginfo->si_pid )
        {   GV_(NXP); Gd( siginfo->si_pid ); // Duplicate process call request.
            _errno = errno_;
            return;
        }
    }
    P shm;
        #ifndef E_flow_Q_process_call_C_alt
    if( !~(N)( shm = E_mem_Q_shared_Q_blk_M( siginfo->si_value.sival_int )))
        #else
    N l = E_text_Z_n_N_s_G( siginfo->si_pid, sizeof( siginfo->si_pid ), 10 );
    C s[ 6 + l + 1 ];
    E_text_Z_s_P_s0_copy( s, "/proc/" );
    E_text_Z_n_N_s( &s[0] + 6 + l, siginfo->si_pid, sizeof( siginfo->si_pid ), 10 );
    *( &s[0] + 6 + l ) = '\0';
    key_t key;
    V1( key = ftok( s, E_flow_Q_process_call_S_ftok_id ))
    {   _errno = errno_;
        return;
    }
    int shm_id;
    if( !~( shm_id = E_mem_Q_shared_R(key))
    || !~(N)( shm = E_mem_Q_shared_Q_blk_M( shm_id ))
    )
        #endif
    {   _errno = errno_;
        return;
    }
    call_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_process_call_srv_S );
    struct E_flow_Q_process_call_srv_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_srv_S, call_id );
    call->process_id = siginfo->si_pid;
    call->shm = shm;
    U_F( E_base_S->E_flow_S_signal, call_req );
    U_F( E_base_S->E_flow_S_signal, wake );
    _errno = errno_;
}
_internal
void
E_flow_Z_signal_V_process_call_reply( int uid
, siginfo_t *siginfo
, P data
){  Dh_();
        #ifndef E_flow_Q_process_call_C_alt
    if( siginfo->si_code != SI_QUEUE )
        #else
    if( siginfo->si_code != SI_USER )
        #endif
        return;
    int errno_ = _errno;
    for_each( call_id, E_base_S->E_flow_Q_process_call_cli_S, E_mem_Q_tab )
    {   struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, call_id );
        if( call->process_id == siginfo->si_pid )
        {   U_L( call->state, active );
            U_L( call->state, ping );
            struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, call->task_id );
            task->run_state = E_flow_Z_run_state_S_ready;
            U_F( E_base_S->E_flow_S_signal, wake );
            _errno = errno_;
            return;
        }
    }
    GV_(NXP); Gd( siginfo->si_pid ); // Call reply from a unrequested process.
    _errno = errno_;
}
//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
I
E_flow_Q_process_call_M( N l
, P *data
){  N shm_id;
        #ifndef E_flow_Q_process_call_C_alt
    if( !~( shm_id = E_mem_Q_shared_M(l)))
        #else
    if( !~( shm_id = E_mem_Q_shared_M_key( E_base_S->E_flow_Q_process_call_S_shm_key, l )))
        #endif
        return ~0;
    P shm;
    if( !~(N)( shm = E_mem_Q_shared_Q_blk_M( shm_id )))
    {   E_mem_Q_shared_W( shm_id );
        return ~0;
    }
    _sigprocmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
    I call_id = E_mem_Q_tab_I_add( E_base_S->E_flow_Q_process_call_cli_S );
    struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, call_id );
    U_L( call->state, active );
    call->shm_id = shm_id;
    call->shm = shm;
    _sigprocmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
    *data = shm;
    return call_id;
}
void
E_flow_Q_process_call_W( I id
){  struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, id );
    E_mem_Q_shared_Q_blk_W( call->shm );
    E_mem_Q_shared_W( call->shm_id );
    _sigprocmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
    E_mem_Q_tab_I_remove( E_base_S->E_flow_Q_process_call_cli_S, id );
    _sigprocmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_reply_pong, 0 );
}
//------------------------------------------------------------------------------
B
E_flow_Q_process_call_I( I id
, pid_t process_id
, B *successful
){  J_assert( process_id > 0 );
    struct E_flow_Q_process_call_cli_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_cli_S, id );
    call->task_id = E_base_S->E_flow_Q_task_S_current;
    call->process_id = process_id;
    struct E_flow_Q_task_Z *task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    task->run_state = E_flow_Z_run_state_S_waiting_for_call_reply;
    _gettime( &task->run_state_time );
    _timeradd( &task->run_state_time, &E_base_S->E_flow_Q_process_call_S_ping_period, &task->run_state_time );
        #ifndef E_flow_Q_process_call_C_alt
    union sigval sv;
    sv.sival_int = call->shm_id;
    U_F( call->state, active );
    V0( sigqueue( process_id, SIGUSR1, sv ))
        #else
    V0( kill( process_id, SIGUSR1 ))
        #endif
    {   U_L( call->state, active );
        *successful = no;
        return no;
    }
    B ret = E_flow_Q_task_I_schedule();
    task = E_mem_Q_tab_R( E_base_S->E_flow_Q_task_S, E_base_S->E_flow_Q_task_S_current );
    *successful = !U_R( call->state, ping );
    return ret;
}
//------------------------------------------------------------------------------
D( flow, call_srv )
{   X_M_( flow, call_req );
    I_D
    {   X_B( flow, call_req, 0 )
            break;
        _sigprocmask( SIG_BLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_req, 0 );
        for_each_pop( call_id, E_base_S->E_flow_Q_process_call_srv_S, E_mem_Q_tab )
        {   struct E_flow_Q_process_call_srv_Z *call = E_mem_Q_tab_R( E_base_S->E_flow_Q_process_call_srv_S, call_id );
            E_flow_Q_process_call_I_func( call->shm );
            E_mem_Q_shared_Q_blk_W( call->shm );
                #ifndef E_flow_Q_process_call_C_alt
            union sigval sv;
            V0( sigqueue( call->process_id, SIGUSR2, sv )){}
                #else
            V0( kill( call->process_id, SIGUSR2 )){}
                #endif
        }
        _sigprocmask( SIG_UNBLOCK, &E_base_S->E_flow_Z_sigset_S_process_call_req, 0 );
    }
    X_W( flow, call_req );
}
/******************************************************************************/
